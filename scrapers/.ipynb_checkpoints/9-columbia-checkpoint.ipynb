{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script serves as a basic tutorial for extracting courses of interest from a university. This is by no means the only (or even best way) to go about this processâ€”so if you come up with a process that works better, feel free to implement! If you're unfamiliar with any of the libraries, the comments below annotate reasoning behind each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is set up a function for standard preprocessing. It's also useful to list all of the URLs we'll need to send requests to before scraping. We want all courses within a 2 year *academic* calendar (as opposed to an annual calendar). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)\n",
    "\n",
    "#course catalog URLs - 2 academic years \n",
    "#only 2019 is available, Fall(3), Summer(2), Spring(1)\n",
    "# urls array\n",
    "\n",
    "urls = [{\"term\": 'Fall 2019', \"url\":\"?site=Directory_of_Classes&instr=&days=&semes=20191&hour=\"},\n",
    "        {\"term\": 'Summer 2019', \"url\":'?si?site=Directory_of_Classes&instr=&days=&semes=20192&hour='},\n",
    "        {\"term\": 'Spring 2019', \"url\":'?site=Directory_of_Classes&instr=&days=&semes=20193&hour='}]\n",
    "\n",
    "link = 'https://doc.search.columbia.edu/classes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to import our keyword csv, split our keyword lists, and preprocess them. The way the csv is set up, we'll want to split the words that are indicated as technical (`T`) or normative (`N`) and that we've chosen to include (`Y`). You'll notice that preprocessing is useful for some of our words but not for others. Here, we've chosen to manually alter words that are not usefully preprocessed. In this case, it means replacing instances of words that are stemmed to end in i.\n",
    "\n",
    "[regex is a bitch here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"../keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process behind extracting relevant courses works in two steps:\n",
    "1. First, we want to find and extract all courses that contain any instance of a normative keyword.\n",
    "2. Then, we want search within these courses to see if it also contains a technical keyword.\n",
    "\n",
    "We initialize a data frame with columns for all of the course items we want to extract. It probably makes the most sense to standardize these feature names across all university scripts so that they're easier to merge in the final compiled dataset for all universities. Our items of interest are:\n",
    "* The course title: `title`\n",
    "* The department and course number: `dept_num`\n",
    "* The course description: `description`\n",
    "* The number of credits for the course: `credits`\n",
    "* The course instructor: `instructor`\n",
    "* The link to the course syllabus (if applicable): `syllabus`\n",
    "* The university the course is extracted from: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year): `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init dfs\n",
    "columbia = pd.DataFrame(columns=['title','dept_num','description','credits','instructor',\n",
    "                                'syllabus','university','term','keyword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below executes part 1 of our extraction. It's long and kind of messy (sorry), so feel free to play around with the structure if you'd like. The key tasks here are to extract our items of interest based on our search queries and append them to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#roster search for all urls\n",
    "from selenium import webdriver\n",
    "for url in urls:\n",
    "#     print(\"url\", url[\"term\"])\n",
    "    #loop through all normative words and extract relevant elements \n",
    "    for word in normative: \n",
    "        url_keyword = link + word + url['url'] #NOTE:this structure will likely be different between rosters!\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url_keyword)\n",
    "        time.sleep(3)\n",
    "\n",
    "        #the number of reponses\n",
    "        elements = driver.find_elements_by_xpath('//*[@id=\"gsa-search-results\"]/li')\n",
    "        results = len(elements)\n",
    "        print(\"elements\", len(elements))\n",
    "        \n",
    "        #scraping each results\n",
    "        for x in range(0, results):\n",
    "            text = driver.find_element_by_xpath('//*[@id=\"gsa-search-results\"]/li[' + str(x+1) +']/div/h3/a').text\n",
    "            section = driver.find_element_by_xpath('//*[@id=\"gsa-search-results\"]/li[' + str(x+1) +']/div/h3/a/span').text\n",
    "            text.replace(section, '')\n",
    "            \n",
    "            course_link = driver.find_element_by_xpath('//*[@id=\"gsa-search-results\"]/li[' + str(x+1) +']/div/div[2]').text\n",
    "            print(\"course_link\", course_link)\n",
    "            \n",
    "            #open the link\n",
    "            response = requests.get(course_link)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print()\n",
    "            \n",
    "#         dept_nums = [p.get_text() for p in soup.select('.title-subjectcode')]\n",
    "#         descs = [p.get_text() for p in soup.select('.course-descr')]\n",
    "#         credit = [p.get_text() for p in soup.select('.credit-val')]\n",
    "#         profs = [p.get_text() for p in soup.select('.instructors span.tooltip-iws')]\n",
    "#         syllabi = [p.get_text() for p in soup.select('.enrlgrp-syllabi')] #TODO: extract syllabi link\n",
    "#         syllabi = [p.replace('Syllabi:\\n','') for p in syllabi] \n",
    "        uni = ['columbia university']\n",
    "        term = url[\"term\"]  \n",
    "        keyword = [word]\n",
    "#         for a,b,c,d,e,f,g,h,i in zip(titles,dept_nums,descs,credit,profs,syllabi,uni,term,keyword):\n",
    "#             columbia = collumbia.append({'title': a, \n",
    "#                                         'dept_num': b,\n",
    "#                                         'description': c,\n",
    "#                                         'credits': d,\n",
    "#                                         'instructor': e,\n",
    "#                                         'syllabus': f,\n",
    "#                                         'university': g,\n",
    "#                                         'term': h,\n",
    "#                                         'keyword': i}, ignore_index=True)\n",
    "        print('url_keyword', url_keyword)\n",
    "#         print('titles', titles)\n",
    "#         print('soup', soup)\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "columbia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all courses containing a normative keyword of interest, we need to filter our courses to only return titles that contain a normative AND a technical keyword. This is the case for all words except instances of our preprocessed `privac` and `secur`, for which we want to return all courses, even if they don't contain two keywords. To do this, we'll split the courses into two data frames, apply our respective conditions, and then merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cornell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bacf091623c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcornell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcornell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'privac'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcornell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'secur'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cornell' is not defined"
     ]
    }
   ],
   "source": [
    "exceptions = columbia.loc[(cornell['keyword']=='privac') | (columbia['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = cornell[cornell['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above cell is likely not the best nor most simple way to execute this step! Feel free to take special liberties here. It's probably wise to pick out a few titles that you know should be returned manually, then check to see if the script is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine dfs \n",
    "columbia = pd.concat([df, exceptions])\n",
    "columbia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to export our csv. Ideally, all csv files should be written to the courses directory in our repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "columbia.to_csv('../courses/columbia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
