{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rice University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script serves as a basic tutorial for extracting courses of interest from a university. This is by no means the only (or even best way) to go about this processâ€”so if you come up with a process that works better, feel free to implement! If you're unfamiliar with any of the libraries, the comments below annotate reasoning behind each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is set up a function for standard preprocessing. It's also useful to list all of the URLs we'll need to send requests to before scraping. We want all courses within a 2 year *academic* calendar (as opposed to an annual calendar). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to import our keyword csv, split our keyword lists, and preprocess them. The way the csv is set up, we'll want to split the words that are indicated as technical (`T`) or normative (`N`) and that we've chosen to include (`Y`). You'll notice that preprocessing is useful for some of our words but not for others. Here, we've chosen to manually alter words that are not usefully preprocessed. In this case, it means replacing instances of words that are stemmed to end in i.\n",
    "\n",
    "[regex is a bitch here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"../keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process behind extracting relevant courses works in two steps:\n",
    "1. First, we want to find and extract all courses that contain any instance of a normative keyword.\n",
    "2. Then, we want search within these courses to see if it also contains a technical keyword.\n",
    "\n",
    "We initialize a data frame with columns for all of the course items we want to extract. It probably makes the most sense to standardize these feature names across all university scripts so that they're easier to merge in the final compiled dataset for all universities. Our items of interest are:\n",
    "* The course title: `title`\n",
    "* The department and course number: `dept_num`\n",
    "* The course description: `description`\n",
    "* The number of credits for the course: `credits`\n",
    "* The course instructor: `instructor`\n",
    "* The link to the course syllabus (if applicable): `syllabus`\n",
    "* The university the course is extracted from: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year): `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "Fall Semester 2017\n",
      "--------------------\n",
      "account\n",
      "--------------------\n",
      "critic\n",
      "--------------------\n",
      "democra\n",
      "--------------------\n",
      "discrimin\n",
      "--------------------\n",
      "equal\n",
      "--------------------\n",
      "equit\n",
      "--------------------\n",
      "ethic\n",
      "--------------------\n",
      "fair\n",
      "--------------------\n",
      "femin\n",
      "--------------------\n",
      "gender\n",
      "--------------------\n",
      "govern\n",
      "--------------------\n",
      "histor\n"
     ]
    }
   ],
   "source": [
    "rice_list = []\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#course catalog URLs - 2 academic years \n",
    "terms = ['Fall Semester 2017', \n",
    "         'Spring Semester 2018', \n",
    "         'Summer Semester 2018', \n",
    "         'Fall Semester 2018', \n",
    "         'Spring Semester 2019', \n",
    "         'Summer Semester 2019',\n",
    "         'Summer Quadmester 2018',\n",
    "         'Fall Quadmester 2018',\n",
    "         'Winter Quadmester 2019',\n",
    "         'Spring Quadmester 2019']\n",
    "\n",
    "for term in terms:\n",
    "    print('--------------------')\n",
    "    print('--------------------')\n",
    "    print(term)\n",
    "    \n",
    "    for word in normative:\n",
    "        driver.get(\"https://courses.rice.edu/courses/swkscat.main\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        select = Select(driver.find_element_by_xpath('//*[@id=\"p_term\"]'))\n",
    "        select.select_by_visible_text(term)\n",
    "\n",
    "#         # for word in normative:\n",
    "#         subject_field = driver.find_element_by_xpath('//*[@id=\"subj_id\"]') \n",
    "#         subject_select = Select(subject_field)\n",
    "\n",
    "#         #select all subjects\n",
    "#         for subject in subject_field.find_elements_by_tag_name('option'):\n",
    "#             subject_select.select_by_visible_text(subject.text)\n",
    "\n",
    "        text_input = driver.find_element_by_xpath('//*[@id=\"p_onebar\"]')\n",
    "        \n",
    "        print('--------------------')\n",
    "        print(word)\n",
    "        \n",
    "        text_input.send_keys(word)\n",
    "        get_course = driver.find_element_by_xpath('//*[@id=\"p_submit\"]')\n",
    "        get_course.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        all_courses = driver.find_elements_by_xpath('//*[@id=\"searchPage\"]/div/div[5]/div/table/tbody')\n",
    "\n",
    "        dept_nums = driver.find_elements_by_class_name('cls-crs')[1:]\n",
    "        titles = driver.find_elements_by_class_name('cls-ttl')[1:]\n",
    "        instructors = driver.find_elements_by_class_name('cls-ins')[1:]\n",
    "        all_credits = driver.find_elements_by_class_name('cls-crd')[1:]\n",
    "        sessions = driver.find_elements_by_class_name('cls-ses')[1:]\n",
    "             \n",
    "        for dept_num, title, instructor, credits, session in zip(dept_nums, titles, instructors, all_credits, sessions):\n",
    "            rice_dict = {}\n",
    "            rice_dict['dept_num'] = dept_num.text\n",
    "            rice_dict['credits'] = credits.text\n",
    "            rice_dict['instructor'] = instructor.text\n",
    "            rice_dict['title'] = title.text\n",
    "            rice_dict['session'] = session.text\n",
    "#             rice_dict['description'] = description.text\n",
    "            rice_dict['term'] = term\n",
    "            rice_dict['keyword'] = word\n",
    "            rice_dict['university'] = 'rice university'\n",
    "            rice_list.append(rice_dict)\n",
    "            \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rice = pd.DataFrame(rice_list)\n",
    "rice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below executes part 1 of our extraction. It's long and kind of messy (sorry), so feel free to play around with the structure if you'd like. The key tasks here are to extract our items of interest based on our search queries and append them to our data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all courses containing a normative keyword of interest, we need to filter our courses to only return titles that contain a normative AND a technical keyword. This is the case for all words except instances of our preprocessed `privac` and `secur`, for which we want to return all courses, even if they don't contain two keywords. To do this, we'll split the courses into two data frames, apply our respective conditions, and then merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = rice.loc[(rice['keyword']=='privac') | (rice['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = rice[rice['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above cell is likely not the best nor most simple way to execute this step! Feel free to take special liberties here. It's probably wise to pick out a few titles that you know should be returned manually, then check to see if the script is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine dfs \n",
    "rice = pd.concat([df, exceptions])\n",
    "rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rice = rice[['title', 'dept_num', 'credits', 'instructor', 'university', 'term', 'keyword', 'session']]\n",
    "rice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to export our csv. Ideally, all csv files should be written to the courses directory in our repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "rice.to_csv('../courses/22-Rice-University.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
