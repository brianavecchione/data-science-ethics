{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The University of Montana Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword preprocessing and url list of relevant catalog years; 2018-19 and 2017-18. Also a list of departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)\n",
    "\n",
    "#course catalog URLs - 2 academic years \n",
    "urls = ['http://catalog.umt.edu/courses/',\n",
    "        'http://catalog.umt.edu/past-catalogs/2017-2018/courses/']\n",
    "\n",
    "#list of all the departments to search through \n",
    "departments = ['actg/','act/','actv/','amgt/','aast/',\n",
    "               'ahma/','ahms/','ahrc/','ahhs/','ahat/',\n",
    "               'anty/','aasc/','arab/','artz/','arth/',\n",
    "               'astr/','atep/','bch/','biol/','bioe/',\n",
    "               'bioo/','bios/','biob/','bioh/','biom/',\n",
    "               'bmed/','bfin/','bgen/','bmgt/','bmis',\n",
    "               'bmkt/','cswa/','cas/','chmy/','chin/',\n",
    "               'clas/','ccs/','coa/','comx/','csd/',\n",
    "               'chth/','capp/','csci/','cstn/','coun/',\n",
    "               'cp/','crwr/','cjus/','cula/','c_i/',\n",
    "               'danc/','dst/','ddsn/','edec/','erth/',\n",
    "               'ecns/','libm/','edsp/','edu/','edld/',\n",
    "               'eele/','etec/','ecp/','ent/','enli/',\n",
    "               'lit/','easl/','eli/','ensc/','enst/',\n",
    "               'fme/','film/','wild/','fors/','frch/',\n",
    "               'egen/','gphy/','geo/','grmn/','gh/','gbld/',\n",
    "               'gyd/','gs/','grk/','hth/','hhp/','hee/','hit/',\n",
    "               'heo/','hprv/','hsta/','hstr/','honr/','htr/',\n",
    "               'hfd/','its/','ids/','irsh/','itln/','jpns/',\n",
    "               'jrnl/','kin/','latn/','law/','leg/','lsci/',\n",
    "               'ling/','mis/','mans/','m/','mba/','mart/',\n",
    "               'mar/','mch/','msl/','mclg/','musi/','muse/',\n",
    "               'must/','nasx/','nrsm/','nrsg/','nutr/','ptrm/',\n",
    "               'phar/','phl/','pt/','phsx/','psci/','psyx/',\n",
    "               'pubh/','ahxr/','rlst/','russ/','scn/','sw/',\n",
    "               'soci/','ssea/','spns/','stat/','ahst/','srvy/',\n",
    "               'nrgy/','thtr/','wldg/','wgss/','writ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of normative and technical keywords lists, the same as in example crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction process for University of Montana:\n",
    "1. Loop through each years' catalog.\n",
    "2. Loop through each of the departments' pages by concatenating the department code to the catalog url. Montana is just like Oregon, all the courses are listed on each departments page.\n",
    "3. On the department page, make a list of all the courses by selecting the class id courseblock. This creates a list where each element is a list containing the full course title and description.\n",
    "4. Loop through all the keywords in the normative list and check to see if the keyword can be found in the full course title.\n",
    "5. If the keyword is in the title, then assign every element of the data columns that can be located.\n",
    "\n",
    "Data columns are defined in the same way as below and have the same anatomy for each course:\n",
    "* The course title - in between '-' and the first occurance of '.' in the full title: `title`\n",
    "* The department and course number - before the first occurance of '.' in the full title: `dept_num`\n",
    "* The course description - the third list element for the course: `description`\n",
    "* The number of credits for the course - before '-' in the full title: `credits`\n",
    "* The course instructor - school does not list in catalog: `instructor`\n",
    "* The link to the course syllabus (if applicable) - school does not list in catalog: `syllabus`\n",
    "* The university the course is extracted from - all from the same university: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year) - For most classes, the text following 'offered' in the description then add the year by matching to the url it came from: `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init dfs\n",
    "montana = pd.DataFrame(columns=['title','dept_num','description','credits','instructor',\n",
    "                                'syllabus','university','term','keyword'])\n",
    "titles = []\n",
    "dept_nums = []\n",
    "descs = []\n",
    "credit = []\n",
    "profs = []\n",
    "syllabi = []\n",
    "uni = []\n",
    "term = []     \n",
    "keyword = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction process. The process to create the table is kept the same as the example crawler, just as a loop on it's own after all the titles, credits, etc. are all gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looping through each years catalog\n",
    "for url in urls:\n",
    "    #looping through all the departments pages to process individual course's information\n",
    "    for dept in departments:\n",
    "        page_link = url + dept\n",
    "        page_response = requests.get(page_link)\n",
    "        soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        courses = [p.get_text().split('\\n') for p in soup.select(\".courseblock\")]\n",
    "        for crs in courses:\n",
    "            title = crs[1]\n",
    "            descr = crs[2]\n",
    "            for word in normative:\n",
    "                if word in title.lower():\n",
    "                    titles.append(title[title.find('-')+2:title.find('.')])\n",
    "                    dept_nums.append(title[:title.find('-')])\n",
    "                    if(descr[descr.find('.')+1:]==''): descs.append('No description available.')\n",
    "                    else: descs.append(descr[descr.find('.')+1:])\n",
    "                    credit.append(title[title.find('.')+2:title.rfind('.')])\n",
    "                    profs.append('Not Listed')\n",
    "                    syllabi.append('Not Listed')\n",
    "                    uni.append('The University of Montana')\n",
    "                    term.append(descr[descr.find(\"Offered\"):descr.find('.')])\n",
    "                    if(url=='http://catalog.umt.edu/courses/'): term[len(term)-1]+= ' in 2018-19'\n",
    "                    else: term[len(term)-1]+= ' in 2017-18'\n",
    "                    keyword.append(word)\n",
    "            \n",
    "for a,b,c,d,e,f,g,h,i in zip(titles,dept_nums,descs,credit,profs,syllabi,uni,term,keyword):\n",
    "    montana = montana.append({'title': a, \n",
    "                              'dept_num': b,\n",
    "                              'description': c,\n",
    "                              'credits': d,\n",
    "                              'instructor': e,\n",
    "                              'syllabus': f,\n",
    "                              'university': g,\n",
    "                              'term': h,\n",
    "                              'keyword': i}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post filtering of course. Code is identical to that of example crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = montana.loc[(montana['keyword']=='privac') | (montana['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = montana[montana['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine dfs \n",
    "montana = pd.concat([df, exceptions])\n",
    "montana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting of code to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "montana.to_csv('39-The University of Montana.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
