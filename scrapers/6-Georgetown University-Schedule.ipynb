{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georgetown University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script serves as a basic tutorial for extracting courses of interest from a university. This is by no means the only (or even best way) to go about this processâ€”so if you come up with a process that works better, feel free to implement! If you're unfamiliar with any of the libraries, the comments below annotate reasoning behind each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is set up a function for standard preprocessing. It's also useful to list all of the URLs we'll need to send requests to before scraping. We want all courses within a 2 year *academic* calendar (as opposed to an annual calendar). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to import our keyword csv, split our keyword lists, and preprocess them. The way the csv is set up, we'll want to split the words that are indicated as technical (`T`) or normative (`N`) and that we've chosen to include (`Y`). You'll notice that preprocessing is useful for some of our words but not for others. Here, we've chosen to manually alter words that are not usefully preprocessed. In this case, it means replacing instances of words that are stemmed to end in i.\n",
    "\n",
    "[regex is a bitch here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"../keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process behind extracting relevant courses works in two steps:\n",
    "1. First, we want to find and extract all courses that contain any instance of a normative keyword.\n",
    "2. Then, we want search within these courses to see if it also contains a technical keyword.\n",
    "\n",
    "We initialize a data frame with columns for all of the course items we want to extract. It probably makes the most sense to standardize these feature names across all university scripts so that they're easier to merge in the final compiled dataset for all universities. Our items of interest are:\n",
    "* The course title: `title`\n",
    "* The department and course number: `dept_num`\n",
    "* The course description: `description`\n",
    "* The number of credits for the course: `credits`\n",
    "* The course instructor: `instructor`\n",
    "* The link to the course syllabus (if applicable): `syllabus`\n",
    "* The university the course is extracted from: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year): `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init dfs\n",
    "georgetown_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "Fall 2017\n",
      "--------------------\n",
      "account\n",
      "0\n",
      "description See individual course for departmental web site, faculty profiles, and course descriptions.\n",
      "\n",
      "Associated Term: Fall 2017\n",
      "Registration Dates: Mar 30, 2017 to Sep 09, 2017\n",
      "Levels: MN or MC Graduate, Undergraduate\n",
      "\n",
      "Main Campus  \n",
      "Lecture Schedule Type\n",
      "0.000 Credits\n",
      "View Course Description\n",
      "View Syllabus\n",
      "View Textbook\n",
      "1\n",
      "description Associated Term: Fall 2017\n",
      "Registration Dates: Mar 30, 2017 to Sep 09, 2017\n",
      "Levels: Undergraduate\n",
      "Attributes: SFS/IECO Finance/Commerce (B), SFS/STIA Growth/Development\n",
      "\n",
      "Main Campus  \n",
      "Lecture Schedule Type\n",
      "3.000 Credits\n",
      "View Course Description\n",
      "View Syllabus\n",
      "View Textbook\n",
      "\n",
      "Scheduled Meeting Times\n",
      "Type Time Days Where Date Range Schedule Type Instructors\n",
      "Lecture 2:00 pm - 3:15 pm MW Healy 103 Aug 30, 2017 - Dec 20, 2017 Lecture Edward Machir\n",
      "2\n",
      "description Lecture\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c3f106a2eaec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mcredits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredit_regex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mgeorgetown_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'credits'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Credits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mgeorgetown_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mgeorgetown_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' (View only)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "georgetown_list = []\n",
    "\n",
    "#course catalog URLs - 2 academic years \n",
    "terms = ['Fall 2017', 'Spring 2018', 'Summer 2018', 'Fall 2018', 'Spring 2019', 'Summer 2019']\n",
    "\n",
    "\n",
    "for term in terms:\n",
    "    print('--------------------')\n",
    "    print('--------------------')\n",
    "    print(term)\n",
    "    term = term + ' (View only)'\n",
    "    \n",
    "    for word in normative:\n",
    "        driver.get(\"https://myaccess.georgetown.edu/pls/bninbp/bwckschd.p_disp_dyn_sched#_ga=\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        select = Select(driver.find_element_by_xpath('//*[@id=\"contentHolder\"]/div[2]/form/table/tbody/tr/td/select'))\n",
    "        submit = driver.find_element_by_xpath('//*[@id=\"id____UID0\"]')\n",
    "        select.select_by_visible_text(term)\n",
    "        submit.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # for word in normative:\n",
    "        subject_field = driver.find_element_by_xpath('//*[@id=\"subj_id\"]') \n",
    "        subject_select = Select(subject_field)\n",
    "\n",
    "        #select all subjects\n",
    "        for subject in subject_field.find_elements_by_tag_name('option'):\n",
    "            subject_select.select_by_visible_text(subject.text)\n",
    "\n",
    "        text_input = driver.find_element_by_xpath('//*[@id=\"title_id\"]')\n",
    "        \n",
    "        print('--------------------')\n",
    "        print(word)\n",
    "        \n",
    "        text_input.send_keys(word)\n",
    "        get_course = driver.find_element_by_xpath('//*[@id=\"id____UID0\"]')\n",
    "        get_course.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        all_courses = driver.find_element_by_xpath('//*[@id=\"contentHolder\"]/div[2]/table[1]/tbody')\n",
    "        courses = all_courses.find_elements_by_tag_name('tr')\n",
    "\n",
    "        full_titles = driver.find_elements_by_class_name('ddtitle')\n",
    "        descriptions = driver.find_elements_by_class_name('dddefault')\n",
    "        \n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        for full_title, description in zip(full_titles, descriptions):\n",
    "            print(counter)\n",
    "            counter += 1\n",
    "\n",
    "#             print(\"full_title\", full_title.text)\n",
    "            print(\"description\", description.text)\n",
    "\n",
    "\n",
    "            georgetown_dict = {}\n",
    "            title_split = full_title.text.split('-')\n",
    "\n",
    "#             for title_el in title_split:\n",
    "#                 print(title_el)\n",
    "                    \n",
    "            dept_num = title_split[0]\n",
    "            georgetown_dict['dept_num'] = title_split[2]\n",
    "            georgetown_dict['title'] = title_split[0]\n",
    "                \n",
    "            credit_regex = r'[0-9]\\.[0-9]{3} Credits'\n",
    "            credits = re.findall(credit_regex, description.text)\n",
    "\n",
    "            georgetown_dict['credits'] = credits[0].replace('Credits', '')\n",
    "            georgetown_dict['description'] = description.text\n",
    "            georgetown_dict['term'] = term.replace(' (View only)', '')\n",
    "            georgetown_dict['keyword'] = word\n",
    "            georgetown_list.append(georgetown_dict)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture\n"
     ]
    }
   ],
   "source": [
    "print(description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>description</th>\n",
       "      <th>keyword</th>\n",
       "      <th>term</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>ACCT 000</td>\n",
       "      <td>See individual course for departmental web sit...</td>\n",
       "      <td>account</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>Accounting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.000</td>\n",
       "      <td>ACCT 001</td>\n",
       "      <td>Associated Term: Fall 2017\\nRegistration Dates...</td>\n",
       "      <td>account</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>Principles of Accounting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  credits    dept_num                                        description  \\\n",
       "0  0.000    ACCT 000   See individual course for departmental web sit...   \n",
       "1  3.000    ACCT 001   Associated Term: Fall 2017\\nRegistration Dates...   \n",
       "\n",
       "   keyword       term                      title  \n",
       "0  account  Fall 2017                Accounting   \n",
       "1  account  Fall 2017  Principles of Accounting   "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "georgetown = pd.DataFrame(georgetown_list)\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "georgetown = pd.DataFrame(georgetown_list)\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below executes part 1 of our extraction. It's long and kind of messy (sorry), so feel free to play around with the structure if you'd like. The key tasks here are to extract our items of interest based on our search queries and append them to our data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all courses containing a normative keyword of interest, we need to filter our courses to only return titles that contain a normative AND a technical keyword. This is the case for all words except instances of our preprocessed `privac` and `secur`, for which we want to return all courses, even if they don't contain two keywords. To do this, we'll split the courses into two data frames, apply our respective conditions, and then merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = georgetown.loc[(georgetown['keyword']=='privac') | (georgetown['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = georgetown[georgetown['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above cell is likely not the best nor most simple way to execute this step! Feel free to take special liberties here. It's probably wise to pick out a few titles that you know should be returned manually, then check to see if the script is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine dfs \n",
    "georgetown = pd.concat([df, exceptions])\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "georgetown = georgetown[['title', 'dept_num', 'description', 'credits', 'term', 'keyword']]\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to export our csv. Ideally, all csv files should be written to the courses directory in our repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "georgetown.to_csv('../courses/6-Georgetown-University-Schedule.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
