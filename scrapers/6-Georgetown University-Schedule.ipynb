{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georgetown University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script serves as a basic tutorial for extracting courses of interest from a university. This is by no means the only (or even best way) to go about this processâ€”so if you come up with a process that works better, feel free to implement! If you're unfamiliar with any of the libraries, the comments below annotate reasoning behind each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we want to do is set up a function for standard preprocessing. It's also useful to list all of the URLs we'll need to send requests to before scraping. We want all courses within a 2 year *academic* calendar (as opposed to an annual calendar). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to import our keyword csv, split our keyword lists, and preprocess them. The way the csv is set up, we'll want to split the words that are indicated as technical (`T`) or normative (`N`) and that we've chosen to include (`Y`). You'll notice that preprocessing is useful for some of our words but not for others. Here, we've chosen to manually alter words that are not usefully preprocessed. In this case, it means replacing instances of words that are stemmed to end in i.\n",
    "\n",
    "[regex is a bitch here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"../keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process behind extracting relevant courses works in two steps:\n",
    "1. First, we want to find and extract all courses that contain any instance of a normative keyword.\n",
    "2. Then, we want search within these courses to see if it also contains a technical keyword.\n",
    "\n",
    "We initialize a data frame with columns for all of the course items we want to extract. It probably makes the most sense to standardize these feature names across all university scripts so that they're easier to merge in the final compiled dataset for all universities. Our items of interest are:\n",
    "* The course title: `title`\n",
    "* The department and course number: `dept_num`\n",
    "* The course description: `description`\n",
    "* The number of credits for the course: `credits`\n",
    "* The course instructor: `instructor`\n",
    "* The link to the course syllabus (if applicable): `syllabus`\n",
    "* The university the course is extracted from: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year): `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init dfs\n",
    "georgetown_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "Fall 2017\n",
      "--------------------\n",
      "account\n",
      "full_title Accounting - 17652 - ACCT 000 - 00\n",
      "full_title Principles of Accounting - 23207 - ACCT 001 - 01\n",
      "full_title Principles of Accounting - 23728 - ACCT 001 - 02\n",
      "full_title Accounting I - 10014 - ACCT 101 - 01\n",
      "full_title Accounting I - 22141 - ACCT 101 - 02\n",
      "full_title Accounting II - 22245 - ACCT 102 - 01\n",
      "full_title Accounting II - 22246 - ACCT 102 - 02\n",
      "full_title Accounting II - 22247 - ACCT 102 - 03\n",
      "full_title Accounting II - 22248 - ACCT 102 - 04\n",
      "full_title Accounting II - 23208 - ACCT 102 - 05\n",
      "full_title Accounting II - 22250 - ACCT 102 - 06\n",
      "full_title Accounting II - 22251 - ACCT 102 - 07\n",
      "full_title Accounting II - 22252 - ACCT 102 - 08\n",
      "full_title Intermediate Accounting I - 10027 - ACCT 201 - 01\n",
      "full_title Intermediate Accounting I - 10028 - ACCT 201 - 02\n",
      "full_title Intermediate Accounting I - 31680 - ACCT 201 - 03\n",
      "full_title Advanced Accounting - 10032 - ACCT 251 - 01\n",
      "full_title Advanced Accounting - 33424 - ACCT 251 - 02\n",
      "full_title Principles of Accounting - 31222 - ACCT 395 - 01\n",
      "full_title Accounting Fundamentals - 24028 - ACCT 500 - 01\n",
      "full_title Accounting Fundamentals - 24029 - ACCT 500 - 02\n",
      "full_title Accounting Fundamentals - 24030 - ACCT 500 - 03\n",
      "full_title Accounting Fundamentals - 24031 - ACCT 500 - 04\n",
      "full_title Accounting Fundamentals - 29213 - ACCT 500 - 05\n",
      "full_title Accounting Fundamentals - 29214 - ACCT 500 - 06\n",
      "full_title Accounting - 29605 - EMBA 805 - 20\n",
      "full_title Business, Accounting & Finance - 30600 - GBUS 400 - 01\n",
      "full_title Business, Accounting & Finance - 30601 - GBUS 400 - 02\n",
      "full_title Business, Accounting & Finance - 30602 - GBUS 400 - 03\n",
      "full_title Basic Accounting for Lawyers - 29369 - LAWG 2086 - 08\n",
      "full_title Income Tax Accounting - 16895 - LAWG 854 - 10\n",
      "full_title Income Tax Accounting - 13641 - LAWG 854 - 11\n",
      "full_title Basic Accounting for Lawyers - 29370 - LAWJ 2086 - 08\n",
      "full_title Accounting for Lawyers - 25954 - LAWJ 300 - 05\n",
      "full_title Financial Accounting - 33461 - MSFO 821 - 200\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "georgetown_list = []\n",
    "\n",
    "#course catalog URLs - 2 academic years \n",
    "terms = ['Fall 2017', 'Spring 2018', 'Summer 2018', 'Fall 2018', 'Spring 2019', 'Summer 2019']\n",
    "\n",
    "\n",
    "for term in terms:\n",
    "    print('--------------------')\n",
    "    print('--------------------')\n",
    "    print(term)\n",
    "    term = term + ' (View only)'\n",
    "    \n",
    "    for word in normative:\n",
    "        driver.get(\"https://myaccess.georgetown.edu/pls/bninbp/bwckschd.p_disp_dyn_sched#_ga=\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        select = Select(driver.find_element_by_xpath('//*[@id=\"contentHolder\"]/div[2]/form/table/tbody/tr/td/select'))\n",
    "        submit = driver.find_element_by_xpath('//*[@id=\"id____UID0\"]')\n",
    "        select.select_by_visible_text(term)\n",
    "        submit.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        # for word in normative:\n",
    "        subject_field = driver.find_element_by_xpath('//*[@id=\"subj_id\"]') \n",
    "        subject_select = Select(subject_field)\n",
    "\n",
    "        #select all subjects\n",
    "        for subject in subject_field.find_elements_by_tag_name('option'):\n",
    "            subject_select.select_by_visible_text(subject.text)\n",
    "\n",
    "        text_input = driver.find_element_by_xpath('//*[@id=\"title_id\"]')\n",
    "        \n",
    "        print('--------------------')\n",
    "        print(word)\n",
    "        \n",
    "        text_input.send_keys(word)\n",
    "        get_course = driver.find_element_by_xpath('//*[@id=\"id____UID0\"]')\n",
    "        get_course.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        all_courses = driver.find_element_by_xpath('//*[@id=\"contentHolder\"]/div[2]/table[1]/tbody')\n",
    "        courses = all_courses.find_elements_by_tag_name('tr')\n",
    "\n",
    "        full_titles = driver.find_elements_by_class_name('ddtitle')\n",
    "        \n",
    "        for full_title in full_titles:\n",
    "            print('full_title', full_title.text)\n",
    "            \n",
    "#         descriptions = courses[1::2]\n",
    "        \n",
    "#         #Has result\n",
    "#         if (len(courses) > 2):\n",
    "#             counter = 0\n",
    "\n",
    "#             for full_title, description in zip(full_titles, descriptions):\n",
    "#                 print(counter)\n",
    "#                 counter += 1\n",
    "\n",
    "#                 georgetown_dict = {}\n",
    "#                 title_split = full_title.text.split('-')\n",
    "                \n",
    "#                 for title_el in title_split:\n",
    "#                     print(title_el)\n",
    "# #                 dept_num = title_split[0]\n",
    "# #                 georgetown_dict['dept_num'] = dept_num\n",
    "                \n",
    "#                 credit_regex = r'[0-9]\\.[0-9]{3} Credits'\n",
    "#                 credits = re.findall(credit_regex, description.text)\n",
    "#                 if len(credits) > 0:\n",
    "#                     georgetown_dict['credits'] = credits[0].replace('Credits', '')\n",
    "\n",
    "#                 title = title_split[1]\n",
    "#                 georgetown_dict['title'] = title\n",
    "#                 georgetown_dict['description'] = description.text\n",
    "#                 georgetown_dict['term'] = term.replace(' (View only)', '')\n",
    "#                 georgetown_dict['keyword'] = word\n",
    "#                 georgetown_list.append(georgetown_dict)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credits</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>description</th>\n",
       "      <th>keyword</th>\n",
       "      <th>term</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>See individual course for departmental web sit...</td>\n",
       "      <td>account</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>17652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.000</td>\n",
       "      <td>Principles of Accounting</td>\n",
       "      <td>Associated Term: Fall 2017\\nRegistration Dates...</td>\n",
       "      <td>account</td>\n",
       "      <td>Fall 2017</td>\n",
       "      <td>23207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  credits                   dept_num  \\\n",
       "0  0.000                 Accounting    \n",
       "1  3.000   Principles of Accounting    \n",
       "\n",
       "                                         description  keyword       term  \\\n",
       "0  See individual course for departmental web sit...  account  Fall 2017   \n",
       "1  Associated Term: Fall 2017\\nRegistration Dates...  account  Fall 2017   \n",
       "\n",
       "     title  \n",
       "0   17652   \n",
       "1   23207   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "georgetown = pd.DataFrame(georgetown_list)\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "georgetown = pd.DataFrame(georgetown_list)\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below executes part 1 of our extraction. It's long and kind of messy (sorry), so feel free to play around with the structure if you'd like. The key tasks here are to extract our items of interest based on our search queries and append them to our data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all courses containing a normative keyword of interest, we need to filter our courses to only return titles that contain a normative AND a technical keyword. This is the case for all words except instances of our preprocessed `privac` and `secur`, for which we want to return all courses, even if they don't contain two keywords. To do this, we'll split the courses into two data frames, apply our respective conditions, and then merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = georgetown.loc[(georgetown['keyword']=='privac') | (georgetown['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = georgetown[georgetown['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above cell is likely not the best nor most simple way to execute this step! Feel free to take special liberties here. It's probably wise to pick out a few titles that you know should be returned manually, then check to see if the script is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combine dfs \n",
    "georgetown = pd.concat([df, exceptions])\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "georgetown = georgetown[['title', 'dept_num', 'description', 'credits', 'term', 'keyword']]\n",
    "georgetown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to export our csv. Ideally, all csv files should be written to the courses directory in our repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "georgetown.to_csv('../courses/6-Georgetown-University-Schedule.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
