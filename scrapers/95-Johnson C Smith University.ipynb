{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Johnson University Catalog Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib.request #handles urls\n",
    "import urllib.parse \n",
    "import linkGrabber #extracts urls\n",
    "import json #encodes/decodes json \n",
    "import csv \n",
    "import requests #downloads a webpage to scrape\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag #beautifulsoup pulls data from HTML\n",
    "import nltk #NLP tasks\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer #removes word endings\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Johnson's catalog is a search page with all the features being javascript interactive modules, I condensed the orinal list urls to just the singular url search page. Even for different terms, the url stays the same so condensing saves time with the loop later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword preprocessing\n",
    "def preprocess(keyword):\n",
    "    keyword = keyword.lower() #lowercase\n",
    "    keyword = word_tokenize(keyword) #tokenize\n",
    "    for word in keyword:\n",
    "        keyword = stemmer.stem(word) #stem \n",
    "    return (keyword)\n",
    "\n",
    "#course catalog URL\n",
    "url = 'https://my.jcsu.edu/ICS/Academics/'\n",
    "\n",
    "#list of all the departments to search through \n",
    "departments = ['AES/','ACC/','XAC/','ACE/','AAS/','AAP/',\n",
    "               'AER/','ARA/','ART/','BAF/','BCT/','BRC/',\n",
    "               'BIL/','BIO/','RTV/','BUS/','XCA/','CW/',\n",
    "               'CRE/','CHE/','CHI/','COM/','CSE/','CSC/',\n",
    "               'COO/','CRM/','DRA/','ECO/','EDU/',\n",
    "               'ELE/','EGR/','ENN/','ENT/','ETH/','FLF/',\n",
    "               'FLS/','FL/','FRE/','FR/','GEN/','GEO/',\n",
    "               'GER/','GBA/','HLT/','HCP/','HIS/','WBA/',\n",
    "               'HON/','CDR/','HDR/','AEN/','CEN/','HEN/',\n",
    "               'HLA/','HLS/','CPO/','HPO/','HRH/','HUM/',\n",
    "               'IDS/','ITA/','JP/','JOU/','JGA/','JGD/',\n",
    "               'ENG/','LAT/','LA/','LS/','LY/','MGT/',\n",
    "               'XMG/','MAR/','MKT/','XMK/','MPL/','XMT/',\n",
    "               'MTH/','MED/','MSC/','MUS/','XHD/','NUR/',\n",
    "               'ORT/','HED/','PHI/','PEH/','PED/','PHS/',\n",
    "               'PHY/','PSC/','XXX/','PLC/','LAW/','POL/',\n",
    "               'RPO/','PSY/','PLS/','PUR/','RT/','RDG/',\n",
    "               'REL/','NSC/','RHC/','RUS/','SCE/','SPL/',\n",
    "               'SMS/','SSC/','SWK/','SOC/','SPA/','SPE/',\n",
    "               'SPM/','LPD/','INS/','SUS/','TEL/','ZZZ/',\n",
    "               'URB/','VBA/','VPD/','VPF/','VPG/','VPM/',\n",
    "               'VPS/','VPT/','VPA/','WEL/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword preprocessing occurs in exactly the same manner as for the example crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'critic', 'democra', 'discrimin', 'equal', 'equit', 'ethic', 'fair', 'femin', 'gender', 'govern', 'histor', 'inequ', 'justic', 'law', 'legal', 'libert', 'moral', 'norm', 'philosoph', 'polit', 'power', 'privac', 'race', 'religi', 'respons', 'right', 'secur', 'social', 'societ', 'surveil', 'transpar', 'valu', 'polic']\n",
      "['^ai', 'algorithm', 'analyt', 'intellig', 'automat', 'code', 'comput', '^cs', 'cyber', 'data', 'digit', '^ict', 'inform', 'intelligen', 'internet', 'machin', '^ml', 'process', '^nlp', 'platform', 'program', 'robot', 'softwar', 'system', 'technolog']\n"
     ]
    }
   ],
   "source": [
    "#import keywords\n",
    "keywords = pd.read_csv(\"keywords.csv\")\n",
    "technical = keywords[(keywords['Technical/Normative']=='T') & (keywords['Include']=='Y')].Keyword\n",
    "normative = keywords[(keywords['Technical/Normative']=='N') & (keywords['Include']=='Y')].Keyword\n",
    "normative = [preprocess(i) for i in normative]\n",
    "technical = [preprocess(i) for i in technical] \n",
    "\n",
    "#replace keywords of interest\n",
    "normative = [w.replace('privaci', 'privac') for w in normative]\n",
    "normative = [w.replace('democraci', 'democra') for w in normative]\n",
    "normative = [w.replace('equiti', 'equit') for w in normative]\n",
    "normative = [w.replace('histori', 'histor') for w in normative]\n",
    "normative = [w.replace('justice', 'justic') for w in normative]\n",
    "normative = [w.replace('liberti', 'libert') for w in normative]\n",
    "normative = [w.replace('philosophi', 'philosoph') for w in normative]\n",
    "normative = [w.replace('societi', 'societ') for w in normative]\n",
    "normative = [w.replace('polici', 'polic') for w in normative]\n",
    "\n",
    "technical = [w.replace('ai', '^ai') for w in technical]\n",
    "technical = [w.replace('cs', '^cs') for w in technical]\n",
    "technical = [w.replace('ict', '^ict') for w in technical]\n",
    "technical = [w.replace('ml', '^ml') for w in technical]\n",
    "technical = [w.replace('nlp', '^nlp') for w in technical]\n",
    "\n",
    "print(normative)\n",
    "print(technical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same idea as the example is followed here:\n",
    "1. First, we want to find and extract all courses that contain any instance of a normative keyword.\n",
    "2. Then, we want search within these courses to see if it also contains a technical keyword.\n",
    "\n",
    "The items of interest remain:\n",
    "* The course title: `title`\n",
    "* The department and course number: `dept_num`\n",
    "* The course description: `description`\n",
    "* The number of credits for the course: `credits`\n",
    "* The course instructor: `instructor`\n",
    "* The link to the course syllabus (if applicable): `syllabus`\n",
    "* The university the course is extracted from: `university`\n",
    "* The term that the course is offered during (fall, spring, summer / year): `term`\n",
    "* The keyword that triggered the extraction (this is for auditing purposes): `keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init dfs\n",
    "johnson = pd.DataFrame(columns=['title','dept_num','description','credits','instructor',\n",
    "                                'syllabus','university','term','keyword','URL'])\n",
    "titles = []\n",
    "dept_nums = []\n",
    "descs = []\n",
    "credit = []\n",
    "profs = []\n",
    "syllabi = []\n",
    "uni = []\n",
    "term = []     \n",
    "keyword = []\n",
    "URL = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 is represented by the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loop through all normative words and extract relevant elements \n",
    "for dept in departments:\n",
    "        page_link = url + dept\n",
    "        page_response = requests.get(page_link)\n",
    "        soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "        courses = [p.get_text() for p in soup.find_all('a')]\n",
    "        links = [r.get('href') for r in soup.find_all('a')]\n",
    "        links = links[:courses.index('Privacy policy')]\n",
    "        courses = courses[:courses.index('Privacy policy')]\n",
    "        for title_pos in range(len(courses)):\n",
    "            for word in normative:\n",
    "                if word in courses[title_pos]:\n",
    "                    classes = links[title_pos][links[title_pos].index('Academics'):]\n",
    "                    page_response2 = requests.get('https://my.jcsu.edu/ICS/'+classes)\n",
    "                    soup2 = BeautifulSoup(page_response2.content, 'html.parser')\n",
    "                    courses2 = [j.get_text() for j in soup2.find_all('a')]\n",
    "                    links2 = [h.get('href') for h in soup2.find_all('a')]\n",
    "                    match = links[title_pos][links[title_pos].index('Academics'):]\n",
    "                    match = match[14:len(match)-1]\n",
    "                    match = match.replace('_',' ')\n",
    "                    links2 = links2[:courses2.index('My Account Info')]\n",
    "                    courses2 = courses2[:courses2.index('My Account Info')]\n",
    "                    for course_pos in range(len(courses2)):\n",
    "                        if match in courses2[course_pos]:\n",
    "                            if any(yr in links2[course_pos] for yr in ['2017','2018','2019']):\n",
    "                                page_response3 = requests.get('https://my.jcsu.edu'+links2[course_pos])\n",
    "                                soup3 = BeautifulSoup(page_response3.content, 'html.parser')\n",
    "                                titles.append(courses[title_pos])\n",
    "                                termInfo = [x.get_text().split('\\n') for x in soup3.find_all('div', attrs={'id': 'TermInfo'})]\n",
    "                                termInfo = termInfo[0]\n",
    "                                faculty = [y.get_text().split('\\n') for y in soup3.find_all('div', attrs={'id': 'Faculty'})]\n",
    "                                faculty = faculty[0]\n",
    "                                courseDescrip = [z.get_text().split('\\n') for z in soup3.find_all('div', attrs={'id': 'CourseDescription'})]\n",
    "                                courseDescrip = courseDescrip[0]\n",
    "                                term.append(termInfo[5])\n",
    "                                dept_nums.append(termInfo[2][termInfo[2].index('(')+1:])\n",
    "                                profs.append(faculty[6])\n",
    "                                syllabi.append('None')\n",
    "                                uni.append('Johnson C Smith University')\n",
    "                                keyword.append(word)\n",
    "                                descs.append(courseDescrip[2])\n",
    "                                credit.append(courseDescrip[2])\n",
    "                                URL.append('https://my.jcsu.edu'+links2[course_pos])\n",
    "\n",
    "for a,b,c,d,e,f,g,h,i,j in zip(titles,dept_nums,descs,credit,profs,syllabi,uni,term,keyword,URL):\n",
    "    johnson = johnson.append({'title': a, \n",
    "                              'dept_num': b,\n",
    "                              'description': c,\n",
    "                              'credits': d,\n",
    "                              'instructor': e,\n",
    "                              'syllabus': f,\n",
    "                              'university': g,\n",
    "                              'term': h,\n",
    "                              'keyword': i,\n",
    "                              'URL': j}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all courses containing a normative keyword of interest, we need to filter our courses to only return titles that contain a normative AND a technical keyword. This is the case for all words except instances of our preprocessed `privac` and `secur`, for which we want to return all courses, even if they don't contain two keywords. To do this, we'll split the courses into two data frames, apply our respective conditions, and then merge them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>description</th>\n",
       "      <th>credits</th>\n",
       "      <th>instructor</th>\n",
       "      <th>syllabus</th>\n",
       "      <th>university</th>\n",
       "      <th>term</th>\n",
       "      <th>keyword</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, dept_num, description, credits, instructor, syllabus, university, term, keyword, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exceptions = johnson.loc[(johnson['keyword']=='privac') | (johnson['keyword'] =='secur')]\n",
    "exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beverly\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>description</th>\n",
       "      <th>credits</th>\n",
       "      <th>instructor</th>\n",
       "      <th>syllabus</th>\n",
       "      <th>university</th>\n",
       "      <th>term</th>\n",
       "      <th>keyword</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, dept_num, description, credits, instructor, syllabus, university, term, keyword, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loop through technical keyword list, extract relevant titles\n",
    "for word in technical:\n",
    "    df = johnson[johnson['title'].str.contains(word, flags = re.IGNORECASE)]\n",
    "    df['keyword2'] = word\n",
    "    \n",
    "#join keyword cols\n",
    "df[\"keyword\"] = df[\"keyword\"].map(str) + \",\" + df[\"keyword2\"]\n",
    "df = df.drop(columns=\"keyword2\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the above cell is likely not the best nor most simple way to execute this step! Feel free to take special liberties here. It's probably wise to pick out a few titles that you know should be returned manually, then check to see if the script is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>dept_num</th>\n",
       "      <th>description</th>\n",
       "      <th>credits</th>\n",
       "      <th>instructor</th>\n",
       "      <th>syllabus</th>\n",
       "      <th>university</th>\n",
       "      <th>term</th>\n",
       "      <th>keyword</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, dept_num, description, credits, instructor, syllabus, university, term, keyword, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine dfs \n",
    "johnson = pd.concat([df, exceptions])\n",
    "johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to export our csv. Ideally, all csv files should be written to the courses directory in our repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as csv\n",
    "johnson.to_csv('95-Johnson C Smith University.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
